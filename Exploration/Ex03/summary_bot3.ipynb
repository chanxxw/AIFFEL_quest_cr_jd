{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\envym\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'lambda_1' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_22      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_22        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">960,000</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_22        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ not_equal_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention… │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,153</span> │ lstm_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BahdanauAttention</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)]    │            │ lstm_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_23      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bahdanau_attenti… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_23        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">960,000</span> │ input_layer_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ repeat_vector_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ embedding_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │ lstm_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ lstm_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_23        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ not_equal_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_3  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855,000</span> │ concatenate_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_22      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_22        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │    \u001b[38;5;34m960,000\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_22        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_22 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m), │     \u001b[38;5;34m98,816\u001b[0m │ embedding_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ not_equal_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention… │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m33,153\u001b[0m │ lstm_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mBahdanauAttention\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m1\u001b[0m)]    │            │ lstm_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_23      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bahdanau_attenti… │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_23        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │    \u001b[38;5;34m960,000\u001b[0m │ input_layer_23[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_3 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ repeat_vector_4[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_23 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m), │     \u001b[38;5;34m98,816\u001b[0m │ embedding_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │ lstm_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ lstm_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_23        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_23[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ not_equal_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_3  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15000\u001b[0m) │  \u001b[38;5;34m3,855,000\u001b[0m │ concatenate_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,005,785</span> (22.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,005,785\u001b[0m (22.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,005,785</span> (22.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,005,785\u001b[0m (22.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\envym\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_186', 'keras_tensor_192']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 200ms/step - accuracy: 0.3119 - loss: 6.4703 - val_accuracy: 0.3285 - val_loss: 5.6641\n",
      "Epoch 2/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 199ms/step - accuracy: 0.3274 - loss: 5.4231 - val_accuracy: 0.3309 - val_loss: 5.5971\n",
      "Epoch 3/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 199ms/step - accuracy: 0.3312 - loss: 5.2053 - val_accuracy: 0.3337 - val_loss: 5.5172\n",
      "Epoch 4/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 196ms/step - accuracy: 0.3413 - loss: 4.8981 - val_accuracy: 0.3370 - val_loss: 5.4653\n",
      "Epoch 5/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 196ms/step - accuracy: 0.3470 - loss: 4.6319 - val_accuracy: 0.3406 - val_loss: 5.4275\n",
      "Epoch 6/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 197ms/step - accuracy: 0.3584 - loss: 4.3132 - val_accuracy: 0.3447 - val_loss: 5.3811\n",
      "Epoch 7/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 203ms/step - accuracy: 0.3693 - loss: 4.0075 - val_accuracy: 0.3470 - val_loss: 5.3837\n",
      "Epoch 8/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 196ms/step - accuracy: 0.3866 - loss: 3.6918 - val_accuracy: 0.3466 - val_loss: 5.3971\n",
      "Epoch 9/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 195ms/step - accuracy: 0.4089 - loss: 3.3875 - val_accuracy: 0.3480 - val_loss: 5.4433\n",
      "Epoch 10/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 196ms/step - accuracy: 0.4436 - loss: 3.1008 - val_accuracy: 0.3467 - val_loss: 5.4857\n",
      "Epoch 11/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 196ms/step - accuracy: 0.4844 - loss: 2.8246 - val_accuracy: 0.3442 - val_loss: 5.5467\n",
      "Epoch 12/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 196ms/step - accuracy: 0.5180 - loss: 2.5846 - val_accuracy: 0.3444 - val_loss: 5.6023\n",
      "Epoch 13/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 196ms/step - accuracy: 0.5546 - loss: 2.3485 - val_accuracy: 0.3439 - val_loss: 5.6708\n",
      "Epoch 14/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 194ms/step - accuracy: 0.5925 - loss: 2.1313 - val_accuracy: 0.3455 - val_loss: 5.7152\n",
      "Epoch 15/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 194ms/step - accuracy: 0.6176 - loss: 1.9770 - val_accuracy: 0.3426 - val_loss: 5.7739\n",
      "Epoch 16/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 195ms/step - accuracy: 0.6469 - loss: 1.7964 - val_accuracy: 0.3434 - val_loss: 5.8282\n",
      "Epoch 17/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 195ms/step - accuracy: 0.6715 - loss: 1.6582 - val_accuracy: 0.3402 - val_loss: 5.8945\n",
      "Epoch 18/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 195ms/step - accuracy: 0.7004 - loss: 1.5125 - val_accuracy: 0.3415 - val_loss: 5.9427\n",
      "Epoch 19/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 194ms/step - accuracy: 0.7215 - loss: 1.3963 - val_accuracy: 0.3418 - val_loss: 5.9979\n",
      "Epoch 20/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 194ms/step - accuracy: 0.7461 - loss: 1.2755 - val_accuracy: 0.3405 - val_loss: 6.0644\n",
      "Epoch 21/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 196ms/step - accuracy: 0.7632 - loss: 1.1897 - val_accuracy: 0.3402 - val_loss: 6.1270\n",
      "Epoch 22/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 195ms/step - accuracy: 0.7852 - loss: 1.0883 - val_accuracy: 0.3390 - val_loss: 6.1917\n",
      "Epoch 23/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 196ms/step - accuracy: 0.8064 - loss: 0.9862 - val_accuracy: 0.3384 - val_loss: 6.2493\n",
      "Epoch 24/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 196ms/step - accuracy: 0.8262 - loss: 0.8907 - val_accuracy: 0.3391 - val_loss: 6.3226\n",
      "Epoch 25/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 195ms/step - accuracy: 0.8432 - loss: 0.8127 - val_accuracy: 0.3347 - val_loss: 6.3953\n",
      "Epoch 26/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 195ms/step - accuracy: 0.8566 - loss: 0.7493 - val_accuracy: 0.3386 - val_loss: 6.4359\n",
      "Epoch 27/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 194ms/step - accuracy: 0.8715 - loss: 0.6814 - val_accuracy: 0.3320 - val_loss: 6.5370\n",
      "Epoch 28/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 194ms/step - accuracy: 0.8811 - loss: 0.6345 - val_accuracy: 0.3342 - val_loss: 6.5617\n",
      "Epoch 29/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 193ms/step - accuracy: 0.8972 - loss: 0.5654 - val_accuracy: 0.3327 - val_loss: 6.6526\n",
      "Epoch 30/30\n",
      "\u001b[1m492/492\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 194ms/step - accuracy: 0.9066 - loss: 0.5210 - val_accuracy: 0.3322 - val_loss: 6.6930\n",
      "모델과 토크나이저 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.layers import Layer, TimeDistributed, RepeatVector, Reshape, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# CSV 파일 경로 (로컬에서 사용하는 경로로 수정)\n",
    "csv_file_path = './news_summary_more.csv'  # CSV 파일 경로를 로컬에 맞게 수정하세요.\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 'text'와 'headlines' 컬럼만 사용\n",
    "text_data = df['text'].astype(str).values  # 'text' 컬럼\n",
    "summary_data = df['headlines'].astype(str).values  # 'headlines' 컬럼\n",
    "\n",
    "# 데이터셋 샘플링 (데이터 크기를 줄여 Colab 리소스 절약)\n",
    "sample_size = int(len(text_data) * 0.1)  # 데이터의 10%만 샘플링\n",
    "text_data = text_data[:sample_size]\n",
    "summary_data = summary_data[:sample_size]\n",
    "\n",
    "# 데이터셋 분리 (80% train, 20% validation)\n",
    "text_train, text_val, summary_train, summary_val = train_test_split(\n",
    "    text_data, summary_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 토크나이저 생성 및 훈련\n",
    "MAX_TEXT_LEN = 50  # 줄여서 리소스 절약\n",
    "MAX_SUMMARY_LEN = 15  # 줄여서 리소스 절약\n",
    "VOCAB_SIZE = 15000  # 어휘 크기 증가 (기존 10000에서 15000으로)\n",
    "\n",
    "# 텍스트 토크나이저\n",
    "text_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<UNK>')  # OOV 토큰을 추가하여 범위 외의 단어 처리\n",
    "text_tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "# 요약 토크나이저\n",
    "summary_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<UNK>')\n",
    "summary_tokenizer.fit_on_texts(summary_train)\n",
    "\n",
    "# 토큰화 및 시퀀스 변환\n",
    "text_train_seq = text_tokenizer.texts_to_sequences(text_train)\n",
    "text_val_seq = text_tokenizer.texts_to_sequences(text_val)\n",
    "\n",
    "# 인덱스를 VOCAB_SIZE 이하로 필터링\n",
    "text_train_seq = [[token if token < VOCAB_SIZE else text_tokenizer.word_index['<UNK>'] for token in seq] for seq in text_train_seq]\n",
    "text_val_seq = [[token if token < VOCAB_SIZE else text_tokenizer.word_index['<UNK>'] for token in seq] for seq in text_val_seq]\n",
    "\n",
    "# 시퀀스 패딩\n",
    "text_train_seq = pad_sequences(text_train_seq, maxlen=MAX_TEXT_LEN, padding='post')\n",
    "text_val_seq = pad_sequences(text_val_seq, maxlen=MAX_TEXT_LEN, padding='post')\n",
    "\n",
    "summary_train_seq = summary_tokenizer.texts_to_sequences(summary_train)\n",
    "summary_val_seq = summary_tokenizer.texts_to_sequences(summary_val)\n",
    "\n",
    "# 인덱스를 VOCAB_SIZE 이하로 필터링\n",
    "summary_train_seq = [[token if token < VOCAB_SIZE else summary_tokenizer.word_index['<UNK>'] for token in seq] for seq in summary_train_seq]\n",
    "summary_val_seq = [[token if token < VOCAB_SIZE else summary_tokenizer.word_index['<UNK>'] for token in seq] for seq in summary_val_seq]\n",
    "\n",
    "# 시퀀스 패딩\n",
    "summary_train_seq = pad_sequences(summary_train_seq, maxlen=MAX_SUMMARY_LEN, padding='post')\n",
    "summary_val_seq = pad_sequences(summary_val_seq, maxlen=MAX_SUMMARY_LEN, padding='post')\n",
    "\n",
    "# 요약 시퀀스에서 시작 토큰 및 종료 토큰 추가\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "\n",
    "# 시작 및 종료 토큰 인덱스 추가\n",
    "start_token_idx = len(summary_tokenizer.word_index) + 1\n",
    "end_token_idx = len(summary_tokenizer.word_index) + 2\n",
    "\n",
    "summary_tokenizer.word_index[START_TOKEN] = start_token_idx\n",
    "summary_tokenizer.word_index[END_TOKEN] = end_token_idx\n",
    "summary_tokenizer.index_word[start_token_idx] = START_TOKEN\n",
    "summary_tokenizer.index_word[end_token_idx] = END_TOKEN\n",
    "\n",
    "# 디코더 입력 데이터 및 타겟 데이터 생성\n",
    "decoder_input_train = np.zeros((len(summary_train), MAX_SUMMARY_LEN))\n",
    "decoder_target_train = np.zeros((len(summary_train), MAX_SUMMARY_LEN))\n",
    "\n",
    "for i, seq in enumerate(summary_train_seq):\n",
    "    decoder_input_train[i, 0] = start_token_idx\n",
    "    decoder_input_train[i, 1:] = seq[:-1]  # Shift to create input\n",
    "    decoder_target_train[i, :] = seq\n",
    "\n",
    "# 검증 데이터에 대해서도 디코더 입력과 타겟 데이터 생성\n",
    "decoder_input_val = np.zeros((len(summary_val), MAX_SUMMARY_LEN))\n",
    "decoder_target_val = np.zeros((len(summary_val), MAX_SUMMARY_LEN))\n",
    "\n",
    "for i, seq in enumerate(summary_val_seq):\n",
    "    decoder_input_val[i, 0] = start_token_idx\n",
    "    decoder_input_val[i, 1:] = seq[:-1]\n",
    "    decoder_target_val[i, :] = seq\n",
    "\n",
    "# Bahdanau Attention 클래스 정의\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query: Decoder hidden state (batch_size, hidden_size)\n",
    "        # values: Encoder outputs (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # Expand query to (batch_size, 1, hidden_size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # Score 계산\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # Attention weights 계산\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context vector 계산\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 모델 정의 (Attention 추가)\n",
    "EMBEDDING_DIM = 64  # 줄여서 리소스 절약\n",
    "HIDDEN_UNITS = 128  # 줄여서 리소스 절약\n",
    "\n",
    "# 인코더 정의\n",
    "encoder_inputs = Input(shape=(MAX_TEXT_LEN,))\n",
    "encoder_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# 디코더 정의\n",
    "decoder_inputs = Input(shape=(MAX_SUMMARY_LEN,))\n",
    "decoder_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(HIDDEN_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention 적용\n",
    "attention = BahdanauAttention(HIDDEN_UNITS)\n",
    "context_vector, _ = attention(state_h, encoder_outputs)\n",
    "\n",
    "# context_vector의 차원을 디코더 출력의 타임스텝 수와 맞추기 위해 RepeatVector 사용\n",
    "context_vector = RepeatVector(MAX_SUMMARY_LEN)(context_vector)\n",
    "\n",
    "# context_vector의 차원을 디코더 출력과 일치시키기 위해 Reshape 사용\n",
    "context_vector = Reshape((MAX_SUMMARY_LEN, HIDDEN_UNITS))(context_vector)\n",
    "\n",
    "# 데이터 유형을 맞추기 위해 Lambda 레이어를 사용하여 float32로 변환\n",
    "context_vector = Lambda(lambda x: tf.cast(x, dtype=tf.float32))(context_vector)\n",
    "decoder_outputs = Lambda(lambda x: tf.cast(x, dtype=tf.float32))(decoder_outputs)\n",
    "\n",
    "# 디코더 출력과 Attention 결과 결합\n",
    "decoder_concat_input = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
    "\n",
    "# Dense Layer\n",
    "decoder_dense = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# 모델 컴파일 및 학습\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(\n",
    "    [text_train_seq, decoder_input_train], \n",
    "    np.expand_dims(decoder_target_train, -1), \n",
    "    epochs=30,  # 에포크 수를 줄여서 리소스 절약\n",
    "    batch_size=16,  # 배치 크기를 줄여서 리소스 절약\n",
    "    validation_data=([text_val_seq, decoder_input_val], np.expand_dims(decoder_target_val, -1))\n",
    ")\n",
    "\n",
    "# 모델 및 토크나이저 저장\n",
    "model.save(\"news_summary_model_with_attention.keras\")\n",
    "with open('text_tokenizer_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(text_tokenizer, f)\n",
    "with open('summary_tokenizer_V3.pkl', 'wb') as f:\n",
    "    pickle.dump(summary_tokenizer, f)\n",
    "\n",
    "print(\"모델과 토크나이저 저장 완료!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
